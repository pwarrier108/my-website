---
title: "Early Telecommunications Era: Building on Constraints"
type: reflection
date: 2025-01-10
category: telecom
themes: ["telecommunications", "networking", "distributed systems", "protocol design"]
era: "1980s Early Telecommunications"
status: published
summary: How limited bandwidth, proprietary networks, and nascent standards shaped a decade of infrastructure work—and what those constraints taught about building systems that endure.
---

## The Context That Shaped Everything

The 1980s and early 1990s were defined by what we *couldn't* do. Bandwidth was expensive. Networks were proprietary. Standards were emerging but not yet universal. The internet as we know it didn't exist for commercial use. Every connection had to be negotiated, leased, and maintained.

This wasn't a limitation—it was the design space.

## Where It Started: Punch Cards and Prime Minicomputers

At Rensselaer Polytechnic Institute, computing meant the Michigan Terminal System. Mainframes. Punch cards. Batch processing. You wrote your program, punched the cards, submitted the deck, and waited. If there was an error, you started over.

In that same room, a Commodore 64 and a Tandy from Radio Shack made an appearance. Small machines, immediate feedback, no waiting. At the time, they seemed like toys—interesting, but not serious computing.

They belonged to a roommate who liked to tinker. I did not yet understand what they represented.

## First Real Systems: Fault Tolerance For Wall Street

My first professional role was with a financial services firm providing real-time commodity trading systems for the Chicago Board of Trade. The system had to be continuously available, because a ticker tape does not pause for maintenance.

This was my introduction to fault-tolerant computing: Stratus machines with hardware redundancy, Tandem systems with software-based failover, Prime minicomputers handling the message flow. I learned TAL—Tandem's Transaction Application Language—as my first real programming language beyond FORTRAN and BASIC.

More importantly, I learned the difference between *hardware* redundancy (Stratus: duplicate everything, switch on failure) and *software* redundancy (Tandem: design the application to survive node failures). Both worked. Both had trade-offs. Neither was universally better.

The lesson wasn't about the machines. It was about understanding what happens when a system can't afford to stop.

## The Network Problem: Connecting Distributed Mainframes

I left the Wall Street environment to join Dun & Bradstreet's networking division, **DunsNet** in Wilton, CT. The challenge was straightforward to describe and complex to solve: interconnect D&B's credit reporting servers—mainframes scattered across the United States, the United Kingdom, Australia, and other locations—with a cheaper alternative than dedicated point-to-point links.

This meant replacing IBM's SNA (Systems Network Architecture) with X.25 packet-switched networks. It was a whole new way of thinking.

SNA was hierarchical and tightly controlled. X.25 was distributed and needed protocol translation at every boundary. The networks were expensive—leased international lines cost thousands of dollars per month. Every byte mattered. Performance monitoring wasn't optional; it was survival.

We built the infrastructure from the ground up. We wrote the protocol converters, some of it in assembler. We debugged obscure SDLC (Synchronous Data Link Control) timing issues using network analyzers that cost more than cars. We coordinated with field engineers across time zones to diagnose problems that only appeared under specific load conditions. Yet, somehow we made it work.

And then, midway through this work, I encountered something that changed my trajectory: a colleague in a different division showed me an IBM PC/XT.

## The PC as Infrastructure: A Revolutionary Idea in the '80s

In the mid-1980s, PCs were still seen as expensive toys; at best as personal productivity tools—word processing, spreadsheets, maybe a simple database. The idea of using one as a *network node* in a carrier-grade system was unconventional, if not heretical.

But the economics were compelling. A PC cost a fraction of a dedicated communications controller. It could be programmed. It could be upgraded. And if we could make it work, it could do things the traditional infrastructure couldn't.

We built a proprietary real-time operating system that ran on top of DOS. The PC connected to the X.25 network via Eicon cards. It participated as a full node in the network—routing packets, managing virtual circuits, handling routing decisions.

This was cutting-edge work. We built what we called "Enhanced Function Nodes"—PCs that could intelligently participate providing advanced functionality like singe sign-on. A user wanting to pull the credit report on a US company with a subsidiary in the UK could seamlessly get a consolidated report without having to login into two separate mainframes. The customer could do it from the convenience of a single terminal.

The PC wasn't just an isolated piece of equipment anymore. It was infrastructure.

## What We Built: The DunsContract Gateway

One of the most significant projects was **DunsContract**—an international gateway connecting DunsNet's private corporate network to the Tenders Electronic Daily (T.E.D.) database, managed by the European Economic Community. T.E.D. was the official source for government contract opportunities across Europe.

This was the first time DunsNet built a gateway to an *independent* database—not something we controlled, not something on our network. We had to negotiate protocols, handle authentication across organizational boundaries, manage data translation, and provide a user experience that made the complexity invisible.

The team included software developers, project managers, marketing, and field support across the U.S. and U.K. The technical challenge was real, but the organizational coordination was just as demanding. Different groups had different priorities. Standards didn't exist for much of what we needed to do. We wrote the specifications as we built the systems.

I spent three months at DunsNet's European headquarters in London, providing support to field staff, troubleshooting customer implementations in the U.K. and France, and documenting procedures that could work across time zones and technical expertise levels.

It worked. Not perfectly, not immediately, but it worked. And it created a template for future international gateway services.

## The Technical Depths: Z80 Assembler and Protocol Translation

The infrastructure work required going deep into the hardware. We wrote Packet Assembler/Disassembler (PAD) software in Z80 assembly to handle SDLC-to-X.25 protocol conversion. This was low-level work: managing buffers, handling CRC errors, detecting aborts, counting transmit underruns, and tracking frames sent and received. A distinctive optimization was reusing the initialization code space as buffer memory after the PAD booted up—because every kilobyte mattered.

We also developed utilities to monitor line activity, measure response times of IBM 3270 terminals connected through 37X5 communications controllers, generate alerts when thresholds were exceeded (response time spikes, excessive hop counts), and isolate performance bottlenecks in a distributed system where any node could be the source of failure.

To validate changes before deployment, we built simulation and traffic-generation tools on multiple platforms—IBM PCs and Tandem systems—to model network behavior. We modified the PAD software, ran diagnostics, tuned parameters, and iterated until the production system matched simulation results. Often, this meant literally squinting at live network traffic on the small screens of network analyzers to isolate elusive issues.

This wasn't glamorous work. It was slow, methodical, and unforgiving. A single miscalculation in buffer management could crash a node. A timing error in protocol negotiation could bring down an entire circuit. But when it worked, it worked at scale—supporting thousands of users across continents, 24 hours a day.

## What Changed: From Proprietary to Open

By the early 1990s, the landscape was beginning to shift. The commercial internet was emerging. TCP/IP was displacing proprietary protocols. Dial-up speeds were starting to improve as we moved well beyond acoustic couplers. Bandwidth was becoming more available, though never abundant.

I left DunsNet to join Compaq Computer Corporation, a company riding the PC wave I had started to understand years earlier—not as a personal computing story, but as an infrastructure story.

The telecommunications work had taught me something durable: constraints shape systems, but discipline shapes judgment. Fault tolerance, performance monitoring, protocol negotiation, and distributed debugging do not disappear when technology changes—they simply move up the stack.

The next decade would test whether those lessons still held when computing left controlled networks and entered enterprises at scale.

## What the Era Taught

Looking back, the early telecommunications era was about building bridges—between proprietary systems that were never meant to talk to each other, between organizations with different standards and priorities, between the expensive, controlled world of carrier networks and the emerging, chaotic world of PCs and open protocols.

The constraints were real. Bandwidth was limited. Standards were incomplete. Tools were primitive. But constraints breed discipline. When you can't afford waste, you learn to be precise. When you can't rely on abundance, you learn to be clever.

Every system built during this era had to justify its existence under pressure. Nothing was fast enough, cheap enough, or reliable enough by default. Everything had to be measured, optimized, and proven.

That discipline—that necessity to understand systems from the silicon up—shaped how I approached every subsequent era of technology. Cloud scale, mobile computing, AI infrastructure—they all have their own constraints, their own trade-offs, their own boundaries between what's possible and what's practical.

But the lesson remains: the best systems aren't built by ignoring constraints. They're built by understanding them deeply and designing within them with intelligence and care.
